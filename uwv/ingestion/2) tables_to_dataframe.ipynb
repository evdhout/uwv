{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cbsodata\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"C:\\Users\\c.hakker\\OneDrive - VISTA college\\Senior Stuff\\Opleiding Data science\\Data\"\n",
    "\n",
    "def ensure_directory_exists(directory):\n",
    "    \"\"\"Ensure a directory exists; create it if it doesn't.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    \"\"\"Load a JSON file with error handling.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON file {file_path}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, file_path):\n",
    "    \"\"\"Save data to a JSON file with error handling.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "        print(f\"Data successfully saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_period_column(df, perioden_column='Perioden', year_column='Year', quarter_column='Quarter'):\n",
    "    \"\"\"\n",
    "    Process the 'Perioden' column to extract 'Year' and 'Quarter'.\n",
    "    \"\"\"\n",
    "    df[[year_column, quarter_column]] = df[perioden_column].apply(\n",
    "        lambda x: pd.Series(extract_latest_year_and_quarter(x))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latest_year_and_quarter(period_text):\n",
    "    \"\"\"\n",
    "    Extract the latest year and quarter from a period string.\n",
    "    \"\"\"\n",
    "    match = re.findall(r'(\\d{4})\\s*(\\w+)', period_text)\n",
    "    if match:\n",
    "        latest_year, latest_period_part = match[-1]\n",
    "        latest_year = int(latest_year)\n",
    "        latest_quarter = extract_quarter(latest_period_part)\n",
    "        return latest_year, latest_quarter\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quarter(period_part):\n",
    "    \"\"\"\n",
    "    Determine the quarter based on months or phrases like '1e kwartaal'.\n",
    "    \"\"\"\n",
    "    if any(month in period_part for month in ['januari', 'februari', 'maart']):\n",
    "        return 1\n",
    "    elif any(month in period_part for month in ['april', 'mei', 'juni']):\n",
    "        return 2\n",
    "    elif any(month in period_part for month in ['juli', 'augustus', 'september']):\n",
    "        return 3\n",
    "    elif any(month in period_part for month in ['oktober', 'november', 'december']):\n",
    "        return 4\n",
    "    if '1e' in period_part:\n",
    "        return 1\n",
    "    elif '2e' in period_part:\n",
    "        return 2\n",
    "    elif '3e' in period_part:\n",
    "        return 3\n",
    "    elif '4e' in period_part:\n",
    "        return 4\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_to_quarterly(df, period_column='Perioden', value_columns=None):\n",
    "    \"\"\"\n",
    "    Convert monthly data to quarterly by summing numeric columns.\n",
    "    \"\"\"\n",
    "    if value_columns is None:\n",
    "        value_columns = df.select_dtypes(include=[float, int]).columns.tolist()\n",
    "\n",
    "    # Extract year and month from the period column\n",
    "    df['Year'] = df[period_column].apply(lambda x: int(re.search(r'(\\d{4})', x).group()))\n",
    "    df['Month'] = df[period_column].apply(lambda x: extract_month(x))\n",
    "\n",
    "    # Assign quarters based on months\n",
    "    df['Quarter'] = df['Month'].apply(lambda x: (x - 1) // 3 + 1)\n",
    "\n",
    "    # Group by year and quarter\n",
    "    quarterly_df = df.groupby(['Year', 'Quarter'])[value_columns].sum().reset_index()\n",
    "    return quarterly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_month(period_text):\n",
    "    \"\"\"\n",
    "    Extract the month number from a period string.\n",
    "    \"\"\"\n",
    "    months = {\n",
    "        'januari': 1, 'februari': 2, 'maart': 3,\n",
    "        'april': 4, 'mei': 5, 'juni': 6,\n",
    "        'juli': 7, 'augustus': 8, 'september': 9,\n",
    "        'oktober': 10, 'november': 11, 'december': 12\n",
    "    }\n",
    "    for month_name, month_num in months.items():\n",
    "        if month_name in period_text.lower():\n",
    "            return month_num\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_sbi_column(df, primary_column='BedrijfstakkenBranchesSBI2008', backup_column='BedrijfskenmerkenSBI2008'):\n",
    "    \"\"\"\n",
    "    Rename 'BedrijfskenmerkenSBI2008' to 'BedrijfstakkenBranchesSBI2008' if necessary.\n",
    "    \"\"\"\n",
    "    if primary_column not in df.columns and backup_column in df.columns:\n",
    "        df = df.rename(columns={backup_column: primary_column})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_industry(df, industry_column='BedrijfstakkenBranchesSBI2008', valid_industries=None):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame by specific industries.\n",
    "    \"\"\"\n",
    "    if valid_industries is None:\n",
    "        valid_industries = [\n",
    "            \"Q Gezondheids- en welzijnszorg\", \"G Handel\", \"C Industrie\",\n",
    "            \"M Specialistische zakelijke diensten\", \"N Verhuur en overige zakelijke diensten\",\n",
    "            \"O Openbaar bestuur en overheidsdiensten\"\n",
    "        ]\n",
    "    return df[df[industry_column].isin(valid_industries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_year(df, year_column='Year', start_year=2008, end_year=2022):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame by year range.\n",
    "    \"\"\"\n",
    "    return df[(df[year_column] >= start_year) & (df[year_column] <= end_year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_sum(df, group_by_columns):\n",
    "    \"\"\"\n",
    "    Group the DataFrame by specified columns and sum numeric data.\n",
    "    \"\"\"\n",
    "    return df.groupby(group_by_columns).sum(numeric_only=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_columns(df, preferred_columns):\n",
    "    \"\"\"\n",
    "    Reorder DataFrame columns to place preferred ones at the start.\n",
    "    \"\"\"\n",
    "    remaining_columns = [col for col in df.columns if col not in preferred_columns]\n",
    "    return df[preferred_columns + remaining_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the table selection JSON\n",
    "table_selection_path = os.path.join(DATA_DIR, 'table_selection.json')\n",
    "table_data = load_json(table_selection_path)\n",
    "\n",
    "if not table_data:\n",
    "    print(\"Table selection data is missing or invalid. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "excluded_identifiers = ['85663NED']\n",
    "identifier_freq_dict = {\n",
    "    entry['Identifier']: entry['Frequency']\n",
    "    for entry in table_data if entry['Identifier'] not in excluded_identifiers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists for 83451NED. Skipping.\n",
      "File already exists for 85928NED. Skipping.\n",
      "File already exists for 80072ned. Skipping.\n",
      "File already exists for 81589NED. Skipping.\n",
      "File already exists for 83147NED. Skipping.\n",
      "File already exists for 81588NED. Skipping.\n",
      "File already exists for 83149NED. Skipping.\n",
      "File already exists for 83148NED. Skipping.\n"
     ]
    }
   ],
   "source": [
    "def fetch_and_save_tables(identifiers, output_folder=DATA_DIR):\n",
    "    \"\"\"Fetch data for identifiers and save as CSV.\"\"\"\n",
    "    ensure_directory_exists(output_folder)\n",
    "    for identifier in identifiers:\n",
    "        output_file = os.path.join(output_folder, f\"{identifier}.csv\")\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"File already exists for {identifier}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            print(f\"Fetching data for {identifier}...\")\n",
    "            data = pd.DataFrame(cbsodata.get_data(identifier))\n",
    "            data.to_csv(output_file, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {identifier}: {e}\")\n",
    "\n",
    "fetch_and_save_tables(identifier_freq_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and combine tables\n",
    "def process_tables(tables, identifier_dict):\n",
    "    processed_tables = []\n",
    "    for i, table in enumerate(tables):\n",
    "        identifier = list(identifier_dict.keys())[i]\n",
    "        frequency = identifier_dict[identifier]\n",
    "        if frequency == 'Maandelijks':\n",
    "            table = monthly_to_quarterly(table)\n",
    "        table = process_period_column(table)\n",
    "        table = rename_sbi_column(table)\n",
    "        table = filter_by_industry(table)\n",
    "        table = filter_by_year(table)\n",
    "        processed_tables.append(table)\n",
    "\n",
    "    combined_df = pd.concat(processed_tables, ignore_index=True)\n",
    "    grouped_df = group_and_sum(combined_df, ['Year', 'Quarter', 'BedrijfstakkenBranchesSBI2008'])\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: C:\\Users\\c.hakker\\OneDrive - VISTA college\\Senior Stuff\\Opleiding Data science\\Data\\83451NED.csv\n",
      "Loading file: C:\\Users\\c.hakker\\OneDrive - VISTA college\\Senior Stuff\\Opleiding Data science\\Data\\85928NED.csv\n",
      "Loading file: C:\\Users\\c.hakker\\OneDrive - VISTA college\\Senior Stuff\\Opleiding Data science\\Data\\80072ned.csv\n",
      "Loading file: C:\\Users\\c.hakker\\OneDrive - VISTA college\\Senior Stuff\\Opleiding Data science\\Data\\81589NED.csv\n",
      "Loading file: C:\\Users\\c.hakker\\OneDrive - VISTA college\\Senior Stuff\\Opleiding Data science\\Data\\83147NED.csv\n",
      "Loading file: C:\\Users\\c.hakker\\OneDrive - VISTA college\\Senior Stuff\\Opleiding Data science\\Data\\81588NED.csv\n",
      "Loading file: C:\\Users\\c.hakker\\OneDrive - VISTA college\\Senior Stuff\\Opleiding Data science\\Data\\83149NED.csv\n",
      "Loading file: C:\\Users\\c.hakker\\OneDrive - VISTA college\\Senior Stuff\\Opleiding Data science\\Data\\83148NED.csv\n",
      "Final DataFrame saved to C:\\Users\\c.hakker\\OneDrive - VISTA college\\Senior Stuff\\Opleiding Data science\\Data\\merged_tables.csv.\n"
     ]
    }
   ],
   "source": [
    "# Assuming file paths are generated for CSV files\n",
    "file_paths = [os.path.join(DATA_DIR, f\"{identifier}.csv\") for identifier in identifier_freq_dict.keys()]\n",
    "\n",
    "def load_csv_files(file_paths):\n",
    "    \"\"\"\n",
    "    Load CSV files into DataFrames.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            print(f\"Loading file: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "# Load tables from CSV files\n",
    "tables = load_csv_files(file_paths)\n",
    "\n",
    "# Process the loaded tables\n",
    "if tables:  # Check if there are any tables loaded\n",
    "    final_df = process_tables(tables, identifier_freq_dict)\n",
    "\n",
    "    # Save the final combined DataFrame\n",
    "    final_output_path = os.path.join(DATA_DIR, 'merged_tables.csv')\n",
    "    final_df.to_csv(final_output_path, index=False)\n",
    "    print(f\"Final DataFrame saved to {final_output_path}.\")\n",
    "else:\n",
    "    print(\"No tables were loaded. Cannot proceed with processing.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
